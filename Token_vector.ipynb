{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "\n",
    "device = th.device('cuda') if th.cuda.is_available() else th.device('cpu')\n",
    "print(f'Using device: {device}')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Load the data\n",
    "path=r\"C:\\Users\\JOSDA\\Desktop\\Autonomous systems\\Second semester\\NLP\\Project\\GIT\\NLP_project\\Data_set\\best_200songs_per25artist.csv\"\n",
    "data = pd.read_csv(path)\n",
    "lyrics = data['clean_lyrics'].tolist()\n",
    "\n",
    "#tokenize the text\n",
    "tokenized_lyrics = [tokenizer.tokenize(text) for text in lyrics]\n",
    "data['tokenized_lyrics'] = tokenized_lyrics\n",
    "\n",
    "#Convert the tokens to their token IDs\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_lyrics]\n",
    "\n",
    "\n",
    "# Encode the inputs\n",
    "encoded_inputs = tokenizer(lyrics, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "#Move the encoded data to the GPU\n",
    "encoded_inputs = {key: value.to(device) for key, value in encoded_inputs.items()}\n",
    "\n",
    "# add the encoded inputs to the DataFrame\n",
    "data['encoded_inputs'] = encoded_inputs['input_ids'].cpu().tolist()  # Mover de vuelta a la CPU para guardarlas en el DataFrame\n",
    "\n",
    "\n",
    "# Save the updated DataFrame\n",
    "data.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JOSDA\\Desktop\\Autonomous systems\\Second semester\\NLP\\Project\\GIT\\NLP_project\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16  # Adjust dependt on GPU\n",
    "num_batches = len(lyrics) // batch_size + (1 if len(lyrics) % batch_size != 0 else 0)\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "# Procesar en lotes\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(lyrics))\n",
    "    \n",
    "    batch_inputs = {key: value[start_idx:end_idx] for key, value in encoded_inputs.items()}\n",
    "    \n",
    "    with th.no_grad():\n",
    "        outputs = model(**batch_inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :]  # 0 is the index of the [CLS] token\n",
    "        all_embeddings.append(batch_embeddings.cpu())  # move emb to cpu\n",
    "\n",
    "    # free memory gpu\n",
    "    th.cuda.empty_cache()\n",
    "\n",
    "# Cat all embeddings\n",
    "all_embeddings = th.cat(all_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings in the DataFrame\n",
    "\n",
    "data['embeddings'] = all_embeddings.tolist()\n",
    "\n",
    "data.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE LABELS AND STORE THEM IN THE DATAFRAME\n",
    "\n",
    "\n",
    "# Convert embeddings to tensores\n",
    "embeddings = th.tensor(data['embeddings'].tolist())\n",
    "\n",
    "#Create labels from the artist names\n",
    "artist_labels = data['artist'].tolist()\n",
    "artist_to_index = {artist: idx for idx, artist in enumerate(set(artist_labels))}\n",
    "labels = th.tensor([artist_to_index[artist] for artist in artist_labels])\n",
    "\n",
    "#Save the \"labels\" in the dataframe\n",
    "data['labels'] = labels.tolist()\n",
    "data.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: torch.Size([2832, 768])\n",
      "Test embeddings shape: torch.Size([708, 768])\n",
      "Train labels shape: torch.Size([2832])\n",
      "Test labels shape: torch.Size([708])\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and testing sets (FOR NEW TRAINIGS FROM HERE)\n",
    "\n",
    "#load the data\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Convertir los embeddings a tensores\n",
    "embeddings = th.tensor(data['embeddings'].tolist())\n",
    "\n",
    "# Convertir las etiquetas a tensores    \n",
    "labels = th.tensor(data['labels'].tolist())\n",
    "\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_embeddings, test_embeddings, train_labels, test_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Verificar la forma de los conjuntos\n",
    "print(\"Train embeddings shape:\", train_embeddings.shape)\n",
    "print(\"Test embeddings shape:\", test_embeddings.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
